---
type: internal-program
status: draft
created: 2025-12-27
program_name: "INNOMIZE AI Adoption Workshop Series"
purpose: Internal capability building, then client offering
format: 5 sessions, 45-60 mins each, over 5 weeks
audiences: [HR, Admin, Finance, IT]
---

# INNOMIZE AI Adoption Workshop Series

Internal program to build AI capability across teams. Apply internally first, refine, then offer to clients.


## Program Overview

| Aspect | Detail |
|--------|--------|
| **Duration** | 5 sessions over 5 weeks |
| **Session length** | 45-60 minutes |
| **Audiences** | HR, Admin, Finance, IT |
| **Format** | Sharing session (presentation + discussion + exercises) |
| **Outcome** | Team can use AI effectively with clear workflows and validation |


## Discovery: Current State & Concerns

This program was designed based on the following discovery questions and answers.

### Q1: What's the current AI usage level across teams?

**HR/Admin:**
- Tools: Gemini, ChatGPT, LMSYS, LMArena
- Purpose: Create content, brainstorming ideas, generate images, summarization, calculation
- Challenges:
  - Lack of experience to "train" AI (provide good context)
  - AI doesn't understand ideas (especially image generation)
  - Context and memory management
  - Vietnamese language still challenging
  - Vietnam policy/law consultation and summarization is difficult
- Their question: "What tasks are current AIs best suited for?"

**IT:**
- Tools: Cursor, Copilot, ChatGPT (chat, copy-paste workflow)
- Tools bought by: Team members themselves or clients (INNOMIZE has no tool plan yet)
- Purpose: Code generation, code explanation, bug fixing, documentation, summarization of long documents

### Q2: What are the biggest concerns about AI usage?

**1. Workflow and Skills (Primary)**
- If team doesn't understand what "done" looks like, they will create mess and bugs
- This is why Mizemind is being developed: provide skills to create foundations
- Architecture thinking is 10x more important than ever

**2. Validation (Secondary)**
- Without workflow, standards, and best practices: how do we validate and review work?
- Using AI to just "get results and code" creates validation-debt
- Need review workflow before scaling

**3. Data Privacy and Confidentiality**
- Client project data
- Internal data (HR and Finance)
- Clear boundaries needed

**4. Cost (Lower priority)**
- Tool purchasing decisions
- Evaluate ROI before buying

### Q3: What does success look like?

Addressed in concerns above:
- Team understands "done" before starting
- Clear validation workflows exist
- Data boundaries are respected
- Skills developed before tools purchased

### Q4: What will you demo?

- Current personal workflow (Claude Code, content creation)
- Mizemind and the "true context" approach
- Future plans and roadmap

### Q5: Format preference?

- Series over weeks (not one-time workshop)
- 45-60 minutes per session
- 4-5 sessions initially, can extend later
- Sharing session format

### Q6: Will this become a client offering?

Yes. Apply at INNOMIZE first, then build "AI Adoption Workshop" as a service offering.


## The 4 Concerns This Program Addresses

1. **Workflow & Skills** → Sessions 1, 2, 3
   - Understanding "done" before starting
   - Avoiding mess and bugs from unclear expectations

2. **Validation** → Session 3
   - Review workflows
   - Standards and best practices
   - Avoiding "validation-debt"

3. **Data Privacy** → Session 4
   - Client project confidentiality
   - HR/Finance sensitive data
   - Clear boundaries

4. **Cost** → Throughout (use what we have, evaluate before buying)


## Session Structure


### Session 1: AI Foundations

**Theme:** Understanding what AI is and what it's good for
**Duration:** 60 minutes

#### Objectives

- Build shared mental model of how AI works
- Understand generative vs reasoning models
- Answer: "What tasks are AI best suited for?"
- Set expectations for what AI can and cannot do

#### Content Outline

**Part 1: How AI Actually Works (15 mins)**

- AI is pattern matching, not thinking
- Trained on text/images, predicts "what comes next"
- No memory between sessions (unless you build it)
- Confident but not always correct

**Part 2: Generative vs Reasoning Models (15 mins)**

| Generative Models | Reasoning Models |
|-------------------|------------------|
| GPT-4, Gemini, Claude (standard) | Claude with extended thinking, o1, o3 |
| Fast, creative, broad | Slower, deliberate, logical |
| Good for drafting, brainstorming | Good for complex problems, analysis |
| Needs HOW (instructions) | Needs WHAT and WHY (context) |

When to use which:
- Generative: Content creation, first drafts, brainstorming, translation
- Reasoning: Complex analysis, multi-step problems, debugging, planning

**Part 3: What AI Is Best Suited For (20 mins)**

**AI excels at:**
→ First drafts (content, code, documents)
→ Summarization (long documents → key points)
→ Translation and localization
→ Brainstorming and ideation
→ Explanation (make complex things simple)
→ Pattern-based tasks (formatting, data extraction)
→ Tedious but straightforward work

**AI struggles with:**
→ Factual accuracy (especially Vietnam-specific policy/law)
→ Nuanced images (specific vision in your head)
→ Long context memory (loses track in long conversations)
→ Vietnamese language nuance (improving but not perfect)
→ Judgment calls that require domain expertise
→ Knowing what it doesn't know

**Part 4: Discussion (10 mins)**

- What tasks do you currently use AI for?
- What frustrations have you experienced?
- What would you like AI to help with?

#### Takeaways

1. AI is a tool, not a replacement for thinking
2. Match the model to the task (generative vs reasoning)
3. AI is best for first drafts, summarization, and tedious work
4. Always validate AI output, especially for facts and policy



### Session 2: The 5 Skills That Matter

**Theme:** Human skills that determine AI productivity
**Duration:** 60 minutes

#### Objectives

- Learn the 5 skills that make AI useful
- Practice each skill with real examples
- Understand why skills matter more than tools

#### Content Outline

**Opening (5 mins)**

Same tool, same training, different results. Why?

The difference is 5 human skills. These determine whether AI multiplies your capability or your mistakes.

**Skill 1: Problem Framing (10 mins)**

Turn vague into specific.

| Vague | Specific |
|-------|----------|
| "Make this better" | "Reduce response time from 4 hours to 1 hour" |
| "Write a post" | "Write a LinkedIn post announcing X, for audience Y, with tone Z" |
| "Summarize this" | "Extract 5 key points, focus on action items, max 200 words" |

**Exercise:** Take a vague request from your work. Rewrite it as a specific prompt.

**Skill 2: Decomposition (10 mins)**

Split work into verifiable steps.

Bad: "Write a quarterly report"
Good:
1. Gather data from sources A, B, C
2. Verify numbers are correct
3. Analyze trends (what changed, why)
4. Draft summary (max 1 page)
5. Format for stakeholders

Each step can be verified before moving to next. Errors caught early.

**Exercise:** Take a complex task you do. Break it into 5-7 verifiable steps.

**Skill 3: Evaluation (10 mins)**

Know what "done" looks like before you start.

Before prompting AI, ask:
- What must be present in the output?
- What would disqualify it?
- What quality level is required?
- How will I verify it's correct?

Without evaluation criteria, you accept "plausible-looking" output. Plausible ≠ correct.

**Exercise:** Pick an AI task you do regularly. Write 3-5 evaluation criteria.

**Skill 4: Correction (10 mins)**

Fix the 20%, don't regenerate.

When AI output is 80% right:
- ❌ Regenerate (try different prompt, hope for better)
- ✅ Correct (edit the 20% that's wrong)

Regenerating wastes time. Correcting builds understanding.

**Skill 5: Systems Thinking (10 mins)**

Make it easier next time.

After each AI task, ask:
- Can I save this prompt for reuse?
- Can I create a template?
- What worked that I should repeat?
- What failed that I should avoid?

Teams with systems thinking improve over time. Teams without do the same work repeatedly.

**Wrap-up (5 mins)**

These 5 skills determine AI productivity more than tool selection.

Next session: How to build workflows with validation.

#### Takeaways

1. Problem Framing: Vague in → garbage out
2. Decomposition: Big tasks fail, small steps succeed
3. Evaluation: Know "done" before you start
4. Correction: Fix, don't regenerate
5. Systems Thinking: Build assets, not one-off work



### Session 3: Workflow Design & Validation

**Theme:** Building AI workflows with quality gates
**Duration:** 60 minutes

#### Objectives

- Design AI-assisted workflows for your role
- Add validation points to catch errors early
- Avoid "validation-debt" (unverified AI output accumulating)

#### Content Outline

**Part 1: Why Workflow Matters (10 mins)**

Without workflow:
- AI generates output
- You accept it (looks fine)
- Errors accumulate
- Problems discovered later (expensive to fix)

With workflow:
- AI generates output
- Validation happens at defined points
- Errors caught early
- Quality maintained

The goal: **Trust the process, not the output.**

**Part 2: Mapping Your Workflow (15 mins)**

Pick one task you do with AI assistance. Map it:

1. **Input:** What triggers this task? What information do you start with?
2. **AI Step:** What does AI do? What prompt/context does it need?
3. **Validation:** How do you verify AI output is correct?
4. **Human Step:** What do you do after AI generates output?
5. **Output:** What does "done" look like?

**Example: HR - Creating Job Description**

| Step | Who | What |
|------|-----|------|
| Input | Human | Role requirements from hiring manager |
| AI Step | AI | Generate first draft based on requirements + company template |
| Validation | Human | Check: accurate requirements? matches our tone? no prohibited terms? |
| Human Step | Human | Edit for nuance, add team-specific details |
| Output | Human | Final JD ready for posting |

**Exercise:** Map one of your AI-assisted tasks using this structure.

**Part 3: Validation Gates (15 mins)**

Where should validation happen?

**Always validate:**
- Facts and numbers (AI hallucinates)
- Policy/legal references (especially Vietnam-specific)
- Customer-facing content
- Financial calculations
- Anything that's hard to reverse

**Validation methods:**
- Checklist (3-5 must-have criteria)
- Second pair of eyes (peer review)
- Source verification (check against official docs)
- Test case (does it work in practice?)

**Part 4: Avoiding Validation-Debt (10 mins)**

Validation-debt: Unverified AI output accumulating in your work.

Signs of validation-debt:
- "I think AI wrote that, but I'm not sure"
- Errors discovered weeks later
- Can't explain why something is the way it is
- Fear of changing AI-generated content

Prevention:
- Validate before moving to next step
- Document what you verified
- Own the output (your name, your responsibility)

**Part 5: Practical Exercise (10 mins)**

In groups by function (HR, Finance, IT):
1. Pick one common AI task
2. Map the workflow
3. Identify 2-3 validation points
4. Share with group

#### Takeaways

1. Design the workflow before using AI
2. Add validation gates at key points
3. Validate facts, numbers, and policy always
4. You own the output, not AI



### Session 4: Data Privacy & Best Practices

**Theme:** What data never enters AI, and practical guidelines
**Duration:** 45 minutes

#### Objectives

- Establish clear data boundaries
- Understand risks of AI data exposure
- Create practical guidelines for each team

#### Content Outline

**Part 1: How AI Tools Handle Data (10 mins)**

What happens when you paste data into AI?

**Cloud AI (ChatGPT, Gemini, Claude web):**
- Data sent to external servers
- May be used for training (check settings)
- Stored temporarily or permanently
- Outside Vietnam jurisdiction

**Enterprise/API versions:**
- Usually not used for training
- Better data controls
- Still leaves your network

**Local AI:**
- Runs on your computer
- Data stays local
- Limited capability

**Key point:** Assume anything you paste could be seen by others.

**Part 2: What Never Enters AI (15 mins)**

**Absolute boundaries (never paste into AI):**

| Category | Examples |
|----------|----------|
| **Client data** | Source code, credentials, API keys, database contents |
| **Personal data** | Employee records, salaries, health info, ID numbers |
| **Financial data** | Bank accounts, specific transactions, financial reports |
| **Confidential** | NDAs, contracts, strategic plans, pricing |
| **Credentials** | Passwords, tokens, access keys |

**For HR/Admin:**
- No employee personal information
- No salary or compensation data
- No performance reviews with names
- No health or leave records

**For Finance:**
- No bank account numbers
- No specific transaction data
- No client financial details
- No internal financial reports with real numbers

**For IT:**
- No client source code (unless explicitly allowed)
- No credentials or secrets
- No production data
- No security configurations

**Part 3: Safe Alternatives (10 mins)**

**Anonymization:**
- Replace names with "Employee A", "Client B"
- Replace specific numbers with ranges or percentages
- Remove identifying details

**Local tools:**
- For sensitive analysis, consider local AI options
- Slower but data stays on your machine

**Asking for patterns, not data:**
- Instead of pasting data: "How do I analyze employee turnover?"
- Instead of pasting code: "How do I implement authentication in Node.js?"

**Part 4: Guidelines by Team (10 mins)**

**HR/Admin Guidelines:**
1. Never paste employee personal data
2. Anonymize before using AI for document drafting
3. Use AI for templates and formats, add real data manually
4. Vietnam policy: Verify with official sources, AI makes mistakes

**Finance Guidelines:**
1. Never paste real financial data
2. Use AI for formulas, formats, explanations
3. Add real numbers manually after AI drafts
4. Always verify calculations independently

**IT Guidelines:**
1. Check client contract for AI usage permissions
2. Never paste credentials, secrets, production data
3. Generic code questions are fine
4. Client-specific code: get explicit permission

#### Takeaways

1. Assume AI data is not private
2. Know your absolute boundaries (what never enters)
3. Anonymize or use patterns when possible
4. When in doubt, don't paste



### Session 5: Demo & Roadmap

**Theme:** See it in action, plan what's next
**Duration:** 60 minutes

#### Objectives

- See real AI workflow in action
- Understand what's coming
- Commit to personal adoption goals

#### Content Outline

**Part 1: Demo - How I Work with AI (25 mins)**

Live demonstration of:

**Claude Code workflow:**
- How I set up projects with context
- Skills and prompts I've built
- How I validate and review output
- Real example of content/code creation

**Mizemind (preview):**
- What it is and why I'm building it
- How it provides "true context"
- How skills work
- Vision for team usage

**Key observations:**
- The setup took time (workflow first)
- I still validate everything
- The system improves over time
- Tools serve the workflow, not the other way around

**Part 2: INNOMIZE Adoption Plan (15 mins)**

**Phase 1: Foundation (Weeks 1-4)**
- Complete this workshop series ✓
- Each team identifies 1-2 pilot workflows
- Document current state and success criteria

**Phase 2: Pilot (Weeks 5-8)**
- Apply AI to pilot workflows
- Weekly check-ins on what's working
- Collect validation challenges
- Refine guidelines based on real usage

**Phase 3: Expand (Weeks 9-12)**
- Roll out to additional workflows
- Build team-specific prompt libraries
- Document best practices
- Measure impact (time saved, quality maintained)

**Phase 4: Productize (Q2)**
- Package learnings into client offering
- "AI Adoption Workshop" as INNOMIZE service
- Case studies from internal experience

**Part 3: Personal Commitments (15 mins)**

Each person identifies:

1. **One workflow** I will apply AI to in the next 2 weeks
2. **One skill** I will practice (framing, decomposition, evaluation, correction, systems thinking)
3. **One boundary** I will respect (what data never enters AI)

Share with the group. Write it down.

**Part 4: Q&A and Feedback (5 mins)**

- Questions from sessions 1-5
- What was most useful?
- What should we add or change?
- Feedback for client version

#### Takeaways

1. AI adoption is a journey, not an event
2. Start with one workflow, expand from there
3. The INNOMIZE team is the first test case
4. Your feedback shapes the client offering



## Supporting Materials

### For Each Session

- Slide deck (15-20 slides max)
- 1-page handout with key takeaways
- Exercise worksheet

### After Program

- AI Usage Guidelines (1-pager per team)
- Prompt Library (shared, team can contribute)
- Validation Checklists (by task type)
- Monthly check-in format

### Measurement

| Metric | How to Measure |
|--------|----------------|
| Adoption | % of team using AI weekly |
| Quality | Validation issues caught/missed |
| Efficiency | Time saved on pilot workflows |
| Confidence | Self-reported comfort level |



## Next Steps

1. [ ] Review and refine this structure
2. [ ] Create Session 1 materials (slides, handout)
3. [ ] Schedule first session
4. [ ] Identify pilot workflows per team
5. [ ] Set up feedback collection mechanism



## Future: Client Offering

This internal program becomes:

**"AI Adoption Workshop"** - INNOMIZE service offering

| Component | Internal Version | Client Version |
|-----------|------------------|----------------|
| Sessions | 5 sessions | 3-5 sessions (customized) |
| Content | General + INNOMIZE specific | Tailored to client industry |
| Pilot | INNOMIZE workflows | Client workflows |
| Follow-up | Internal check-ins | Consulting engagement |
| Outcome | Team capability | Team capability + case study |

Learnings from INNOMIZE rollout:
- What worked
- What didn't
- Common questions
- Real examples and stories
