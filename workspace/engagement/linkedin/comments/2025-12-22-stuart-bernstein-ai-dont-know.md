---
type: linkedin-engagement
created: '2025-12-22T14:36:00Z'
last_updated: '2025-12-22T14:36:00Z'

author:
  name: "Stuart Bernstein"
  profile: "people/stuart-bernstein.md"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/bernsteinstuart_why-ai-agents-struggle-to-say-i-dont-know-activity-7408854231857868800-eGVc"
  date: '2025-12-22'
  reactions: 3
  comments_count: 2
  reposts: 0
  theme: "ai-uncertainty-honesty"
  angle: "AI confidence vs humility"
  key_points:
    - "AI trained to be helpful means filling gaps even when it shouldn't"
    - "AI won't say 'I don't know' or 'Reads fine to me'"
    - "Overconfidence where humility would be better"
    - "Hope next gen agents treat 'I don't know' as feature"
  hashtags: []

thread_topic: "AI uncertainty and honesty"
topic_tags: [ai-uncertainty, ai-honesty, hallucination, ai-agents]

engagement_status: "posted"
response_received: false
follow_up_needed: false
follow_up_date: null
---

## Original Post

Why AI Agents Struggle to Say "I Don't Know" Among humans, admitting uncertainty often builds trust (a lesson most of us wish we'd learned earlier). AI, on the other hand, is trained to be helpful. And "helpful" almost universally means "filling the gaps", even when it shouldn't. You can see it in the small things: 1. Ask AI a question, it will give you an answer, even when it's guessing. 2. Tell AI, "I wrote this" and it will confidently say, "I've improved it for you." 3. Share an opinion with AI, and it will agree or debate without pause. What it won't do is say, "I don't know", or "Reads fine to me", or "Acknowledge" Instead of leaving space for uncertainty, it wraps neutrality in a paragraph and passes it off as truth with a finely printed disclaimer "AI can make mistakes". Maybe AI has to feel overconfident for us to continue experimenting with it, but it often feels overconfident in places where humility might've been the better move. I'm hopeful the next generation of agents will treat "I don't know" as a feature, not a flaw.

## Notable Comments

```yaml
- id: 1734879360000
  author: "Mar Vincent Jambrich"
  profile: null
  sentiment: "positive"
  content: |
    You may want to read OpenAI's paper titled: Training LLMs for Honesty via Confessions.
  reactions: 0
  insight: "Points to research on AI honesty"
  replies: []

- id: 1734879361000
  author: "Kishan Kumar"
  profile: null
  sentiment: "positive"
  content: |
    We are solving this problem with a cognitive protocol for AI agents called KyroQL at KyroDB.
  reactions: 0
  insight: "Someone building solution for this problem"
  replies: []
```

## Our Engagement

```yaml
- id: 1734879360000
  type: "comment"
  reply_to: null
  status: "posted"
  timestamp: '2025-12-22T14:36:00Z'
  content: |
    Felt this building AI workflows. The confident wrong answer is more dangerous than no answer.

    AI doesn't know your patterns, your decisions, your context. But it will generate code like it does.

    That's why human validation checkpoints matter.
  strategy: "Personal Experience - concise, punchy, connects uncertainty to practical AI workflow experience"

- id: 1734879400000
  type: "reply"
  reply_to: "Stuart Bernstein"
  status: "posted"
  timestamp: '2025-12-22T15:00:00Z'
  content: |
    Yeah, more will come. At least the vocabulary expansion means we're paying attention to the failure modes. That's progress.
  strategy: "Continue conversation - reframes his humor into insight, positions naming problems as first step to solving them"
```

## Notes

- Topic aligns with AI validation and human oversight themes
- Stuart is thoughtful about AI limitations
- Good opportunity to connect on shared AI workflow concerns
