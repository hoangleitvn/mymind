---
type: linkedin-engagement
created: '2025-12-29'
last_updated: '2025-12-29'

author:
  name: "Nuno Donato"
  profile: "people/nunodonato.md"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/nunodonato_interesting-question-and-experiment-have-activity-7408948267146973184-gQ7v"
  date: '2025-12-22'

  thesis: "AI systems validate ideas rather than critically challenge them. Without explicit framing, AI won't call out flawed business concepts."
  insight: "Testing bad ideas on AI without mentioning they're bad results in AI calling them 'amazing and creative' - users get false validation."
  their_angle: "This is a risk for people using AI for business validation"
  our_opening: "Add depth on why this happens (helpful assistant training) and what actually works (force critical lens first)"

  theme: "ai-tools"
  key_points:
    - AI trained to be supportive, not challenging
    - Without framing, AI validates even bad ideas
    - Custom instructions can help
    - Users should be aware of this bias

our_engagement:
  - id: 1735432000000
    type: "comment"
    strategy: "add-depth"
    identity_applied:
      tone: "direct, practical"
      theme_overlap: "practical-engineering"
    timestamp: '2025-12-29'
    content: |
This is the "helpful assistant" problem. AI is trained to be supportive, not challenging. It optimizes for user satisfaction, not truth.

What works: tell it to find reasons the idea will fail before asking if it's good. Force the critical lens first. Or ask it to list the top 3 assumptions that must be true for this to work.

The default mode is encouragement. You have to explicitly request the opposing view.

engagement_status: "posted"
---
