---
type: linkedin-engagement
created: '2025-12-19T04:50:00Z'
last_updated: '2025-12-19T04:50:00Z'

author:
  name: "Sahil Sheikh"
  profile: "people/sahil-sheikh.md"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/sahil-sheikh-ds_dont-hire-an-ai-engineer-if-heshe-doesn-activity-7406204177305153536-4XEc"
  date: '2025-12-16'
  reactions: 169
  comments_count: 10
  reposts: 0
  theme: "AI Engineer hiring practices and skill requirements"
  angle: "Warning against hiring AI Engineers without ML background"
  key_points:
    - "AI Engineering is a trend approaching peak, not sustainable long-term"
    - "90%+ GenAI projects never make production"
    - "When GenAI demand drops, AI Engineers without ML foundation will struggle"
    - "Recruiters should invest in upskilling AI hires on classical ML"
  hashtags: []

thread_topic: "AI Engineer skill requirements debate"
topic_tags: [ai-engineering, hiring, ml-fundamentals, genai, career-advice]

engagement_status: "posted"
response_received: false
follow_up_needed: false
follow_up_date: null
---

## Original Post

Don't hire an "AI Engineer" if he/she doesn't have a strong ML background…. I might get a lot of heat for this, but your AI Engineer might become a liability for you soon…! Hear me out… At present, everything seems fancy as every company is trying to jump into the AI race, 10s of new GenAI use cases each day, everyone wants to look cool and automate their processes with AI agents. For this they are hiring the most random people with an AI Engineer title on their LinkedIn bio (sarcasm). The ones who are buying the hype, you are creating a massive skill debt for yourself! Let me give you a reality check. More than 90% of the Gen AI projects never make it to production. The ones that do, probably don't give you the returns that you expected. "AI Engineering" is a trend, and trends end (Sad but True)… We are approaching the trend peak at a much faster rate. I don't claim it as a bubble or anything, but looking at how stagnant the innovation has become, I can confidently say we are not seeing any major breakthrough in the next couple of years. Now, the day we hit the peak, things would start taking a turn. You'll slowly start witnessing a major reduction in demand for Gen AI based projects in the next 2-3 years (being WAY TOO OPTIMISTIC right now). And I'm pretty sure some major AI Giant would've built something that would make all the AI Engineers' (the fancy title ones) work obsolete. The day there's a reduction in demand, you'll be faced with the challenge to do projects which don't involve agents/LLMs. So take Time Series, Computer Vision, GNNs, Classical ML or even pure stats projects. Every "AI Engineer" who hasn't upskilled to learn at least the basics of these, will suffer! So here's a humble request to the recruiters: if you are hiring AI Engineers thinking AI = LLM, spend more time in upskilling them before it's too late. And to all the AI Engineers without a DS/ML background: Shortcuts can only take you so far, when trends end, only an innovative mind with a strong foundation can save you. You can't create the demand yourself, but you can at least upskill so that when the shift happens, you are already prepared. Do you think GenAI hype would go out of fashion and companies would again give more preference to the ML ones? Let me know in the comments…

## Notable Comments

```yaml
- id: 1734567600000
  author: "Joao Oliveira"
  profile: null
  sentiment: "negative"
  content: |
    I understand your point, but I completely disagree. AI engineers are essentially software engineers who know how to leverage GenAI to build things, mostly using off-the-shelf tools. Data scientists, for the most part, are not necessarily good software engineers. In fact, the best ones I've worked with are far better at solving complex problems using math and statistics than at building e2ed production systems. That's exactly why ML engineers exist: to bridge the gap between notebooks and production. You're comparing apples and oranges here.
  reactions: 13
  insight: "Strong counterpoint - distinguishes AI Engineer (software focus) from Data Scientist (math/stats focus)"
  replies: []

- id: 1734567700000
  author: "Donny Pacheco"
  profile: null
  sentiment: "neutral"
  content: |
    There's truth here, but I think the failure mode is slightly misidentified. Most GenAI projects don't fail because people lack deep ML theory. They fail because organizations confuse model use with problem ownership. You can have perfect ML foundations and still ship nothing if the work isn't tied to real decisions, constraints, and accountability. That said, you're right about skill debt. Titles inflated by trends always get stress-tested when the cycle turns. The engineers who survive aren't the ones chasing LLM wrappers or clinging to "pure ML," but the ones who understand systems end-to-end: data, incentives, deployment, and when not to use AI at all. It's not GenAI vs classical ML. It's shallow execution vs durable understanding. Trends end. Leverage stays.
  reactions: 1
  insight: "Nuanced take - reframes as execution problem, not skills problem"
  replies: []

- id: 1734567800000
  author: "WADDAH A."
  profile: null
  sentiment: "neutral"
  content: |
    "Strong ML background" is a good filter if the role involves training, fine-tuning, modeling choices, metrics, and statistical rigor. But it shouldn't be a blanket rule for every "AI Engineer." In practice, there are two paths: AI Application Engineer (LLM/RAG/agents) and ML Engineer (CV/TS/GNN/classical). The real hiring mistake isn't focusing on "LLM people"; it's hiring without defining the path and measuring success through demos instead of offline evaluations, production KPIs, and drift and quality monitoring.
  reactions: 2
  insight: "Practical framework - distinguishes AI Application Engineer from ML Engineer"
  replies: []
```

## Our Engagement

```yaml
- id: 1734580200000
  type: "comment"
  reply_to: null
  status: "posted"
  timestamp: '2025-12-19T04:50:00Z'
  content: |
    The 90% failure rate is real. But from shipping 50+ products: projects fail because of product vision, GTM, and unclear success metrics. Not because the team lacked ML PhDs.

    We've failed projects with strong teams. 2-3 Masters and PhDs in ML/Data Science. Still failed. Nobody said "the code was bad" or "the team wasn't technical enough." The problems were upstream: unclear problem definition, no production constraints, no ownership.

    We've also shipped successful AI products with engineers who learned ML on the job. The difference? Systems first, model second. Understanding how to build AI-native systems matters more than understanding how transformers work.

    If you want ML skills, hire ML engineers or data scientists. They'll research and write papers. But they rarely ship to production without help from engineers who understand systems, evaluation, and when NOT to use AI at all.
  strategy: "Expert Insight + Personal Experience - Challenges premise with real examples of both failure modes"
```

## Notes

- Post sparked good debate between ML purists vs engineering pragmatists
- Aligns with my positioning on pragmatic engineering over theory
- Good opportunity to add production perspective missing from the discussion
- Joao and Donny's comments are well-reasoned counterpoints worth acknowledging
