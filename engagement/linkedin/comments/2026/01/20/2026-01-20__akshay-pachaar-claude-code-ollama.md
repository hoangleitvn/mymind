---
type: linkedin-engagement
created: '2026-01-20'
last_updated: '2026-01-20'

author:
  name: "Akshay Pachaar"
  linkedin_url: "https://in.linkedin.com/in/akshay-pachaar"
  profile: "people/akshay-pachaar.md"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/akshay-pachaar_this-is-huge-now-you-can-use-claude-code-activity-7419080233217175552-156o"
  activity_id: "7419080233217175552"
  date: '2026-01-20'
  reactions: 1870
  comments_count: 10
  reposts: 0
  theme: "Claude Code with local LLMs via Ollama"
  angle: "Free alternative to Claude API using open-source models"
  key_points:
    - Ollama now compatible with Anthropic messages API
    - Can run Claude Code agentic loops with local models
    - Private LLMs on your own machine
  hashtags: []

thread_topic: "Claude Code with Ollama local LLMs"
topic_tags: [claude-code, ollama, local-llm, ai-development]

engagement_status: "posted"
response_received: false
follow_up_needed: false
follow_up_date: null
---

## Original Post

This is huge. Now you can use Claude Code for FREE: Ollama is now compatible with the anthropic messages API. which means you can use Claude code with open-source models. Think about that for a second. the entire Claude harness: - the agentic loops - the tool use - the coding workflows All powered by private LLMs running on your own machine. Link to the official blog in the first comment.

## Notable Comments

```yaml
- id: 1737362000000
  author: "Maranatha Poirier"
  profile: null
  sentiment: "neutral"
  content: |
While that sounds great, I've not found any open source models that can usably run locally, that compete with Opus 4.5 or even Sonnet. Not even close. While running local LLMs (SLMs actually) is a hot topic, the performance reality and capability gap make it not as useful as it might seem for something as important as coding. On a practical note, it's nice that Ollama is now compatible with the Anthropic API.
  reactions: 88
  insight: "Top comment - reality check on local model capabilities"
  replies: []

- id: 1737362100000
  author: "Garth Arendse"
  profile: null
  sentiment: "negative"
  content: |
What model though ?. None is good enough for agentic coding locally - you might as well pay for tokens using proper AI compute - even a NVidia DGX Spark with a single Blackwell is going to be a disappointment.
  reactions: 6
  insight: "Skeptical about local model quality for agentic work"
  replies: []
```

## Our Engagement

```yaml
- id: 1737362500000
  type: "comment"
  status: "posted"
  sentiment: "positive"
  content: |
Ran this yesterday on my MacBook. Works but slow.

The trade-off is real: Opus 4.5 for speed and quality when it matters, local LLM as fallback for cost-sensitive work. GLM or similar could work for simpler tasks.

Nice to have options.
  strategy: "Add Context - Share actual hands-on experience running it, acknowledge trade-offs that commenters are discussing"
  replies: []
```

## Notes

- High engagement post (1870 reactions)
- Comments mostly skeptical about local model quality
- Our comment adds real hands-on experience vs theoretical discussion
- First engagement with Akshay Pachaar (170K followers, AI/ML content creator)
