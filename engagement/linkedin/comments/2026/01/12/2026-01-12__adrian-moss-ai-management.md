---
type: linkedin-engagement
created: '2026-01-12T08:53:00Z'
last_updated: '2026-01-12T08:53:00Z'

author:
  name: "Adrian Moss"
  linkedin_url: "https://ae.linkedin.com/in/adrianmmoss"
  profile: "people/adrian-moss.md"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/adrianmmoss_claudecode-ai-aimanagement-activity-7416394271622238208-JKWp"
  activity_id: "7416394271622238208"
  date: '2026-01-12'
  reactions: 0
  comments_count: 0
  reposts: 0
  theme: "AI coding reliability and verification"
  angle: "AI Management as emerging discipline"
  key_points:
    - AI coding tools are transformative AND unreliable
    - Claude Code admitted "verify my work, don't trust my claims"
    - Protocols aren't enough - things still slip through
    - AI Management skills: pattern recognition, verification architecture, protocol compliance, healthy skepticism
  hashtags: [claudecode, AI, aimanagement, vibecoding]

thread_topic: "AI Management discipline"
topic_tags: [ai-development, claude-code, verification, vibe-coding, ai-management]

engagement_status: "posted"
response_received: false
follow_up_needed: false
follow_up_date: null
---

## Original Post

If you're relying on the latest AI coding tools - BEWARE. Claude Code told me today: "Verify my work, don't trust my claims." Most users of AI coding tools know they should have protocols documented and referenced at the start of every session - standing instructions to persist learnings and enforce best practices. I have those. Evolved over months and updated with every new "learning experience." But today I discovered a significant bug in production. A core feature I'd explicitly confirmed was working before deployment... wasn't.

When I questioned Claude Code on how this happened - given that mandatory testing before deployment is one of my critical documented protocols - the response was disarmingly honest: "I can't explain it away or promise it won't happen again, because I've made that promise before and broken it. The only honest thing I can say is: verify my work, don't trust my claims."

Let that sink in.

The emerging role: AI Management - Pattern recognition, Verification architecture, Protocol compliance, Healthy scepticism.

The irony? The people best positioned for this role aren't necessarily developers. They're operators who understand systems, workflows, and quality control. People who know that "it's done" and "it works" are two very different things.

## Notable Comments

```yaml
[]
```

## Our Engagement

```yaml
- id: 1736677000000
  type: "comment"
  status: "posted"
  sentiment: "positive"
  content: |
"Verify my work, don't trust my claims" - this should be in every AI tool's onboarding.

The framing I use: treat AI output like a junior developer's PR. Review it, don't merge it. The engineers who catch mistakes early have debugged those problems manually before.

Protocols help. Domain knowledge is the real filter.
  strategy: "Add Context - Reinforce his point with 'junior dev PR' framing from earlier conversation"
  replies: []
```

## Notes

- Uses #vibecoding - directly relevant to our content
- His "AI Management" framing is useful
- Claude Code user sharing real failure mode
- Fresh post (28 min old) - early comment opportunity
- Strong alignment with our AI hallucination patterns content
