---
type: linkedin-engagement
created: '2026-01-31T03:15:00Z'
last_updated: '2026-01-31T03:15:00Z'

author:
  name: "Dharmesh Shah"
  linkedin_url: "https://www.linkedin.com/in/dharmesh"
  profile: "people/dharmesh-shah.md"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/dharmesh_the-scariest-three-words-to-get-as-a-response-activity-7423053580091432961-v9oo"
  activity_id: "7423053580091432961"
  date: '2026-01-31'
  reactions: 350
  comments_count: 10
  reposts: 0
  theme: "AI sycophancy in agentic coding tools"
  angle: "Skepticism about AI agreement patterns - why make mistakes if user is 'absolutely right'"
  key_points:
    - "you're absolutely right" is the scariest response from AI coding tools
    - If user was so right, why did AI make the mistake initially?
    - Skills help provide more context for common tasks
    - Even Opus 4.5 makes surprising mistakes within context
  hashtags: []

thread_topic: "AI sycophancy and agentic coding reliability"
topic_tags: [ai-coding, claude-code, ai-sycophancy, prompt-engineering, agentic-ai]

engagement_status: "draft"
response_received: false
follow_up_needed: false
follow_up_date: null
---

## Original Post

The scariest three words to get as a response from an agentic coding app (like Claude Code): "you're absolutely right". This often comes back quickly when I ask the agent whether a different approach wouldn't be better because it's approach wasn't consistent, it didn't follow the DRY (Don't Repeat Yourself) principle, etc. What worries me is that if I'm so absolutely right -- then why make that mistake in the first place? This is why I try to build skills for my more common tasks (so it has more instructions and context for those things), but given the quality of the Opus 4.5, it surprises me sometimes the level of things it gets wrong -- even when working on the same task when the conversation is still in context (not compacted). Anyways, we still live in magical times, but every now and then, feels like there's a glitch in the Matrix.

## Notable Comments

```yaml
- id: 1738293600000
  author: "Alex Greenshpun"
  profile: null
  sentiment: "positive"
  content: |
THE scariest, for sure. And goes to show that building is 80% planning, especially with agents. For me, planning mode + brainstorming + parallel deep research (+specific MCPs) helps minimize these "compliments". I also like to run things through my panel of experts skill - sometimes with parallel subagents representing each expert.
  reactions: 4
  insight: "Shares practical mitigation: planning mode, MCPs, panel of experts skill"
  replies: []

- id: 1738294200000
  author: "Praveen K."
  profile: null
  sentiment: "neutral"
  content: |
I've started asking the model to explain why it's recommending changes, without me sharing my own opinion first. Sometimes its responses is making me think a bit more.
  reactions: 1
  insight: "Strategy: withhold opinion to get unbiased AI recommendation first"
  replies: []

- id: 1738294800000
  author: "Michael Woodle, M.S."
  profile: null
  sentiment: "negative"
  content: |
I find Opus 4.5 does this A LOT. Very annoying. I've recently switched to Codex 5.2 (high) and find it much better in terms of respectfully disagreeing with me.
  reactions: 1
  insight: "Alternative tool recommendation - Codex 5.2 as less sycophantic"
  replies: []

- id: 1738295400000
  author: "Edwin Manual"
  profile: null
  sentiment: "neutral"
  content: |
The moment I see "you're absolutely right," I know it's time to /compact or /clear.
  reactions: 1
  insight: "Practical trigger for context reset"
  replies: []
```

## Our Engagement

```yaml
- id: 1738296900000
  type: "comment"
  status: "draft"
  sentiment: "positive"
  content: |
I added a rule "Challenge wrong assumptions - Correct me when I'm wrong. Never agree to be polite, because polite agreement compounds errors and leads to chains of wrong decisions" as non-negotiable. But sometimes AI still escapes the rule, even simple rules like commit. So one rule here is zero-trust, verify everything, ask questions.
  strategy: "Add Context - Share actual CLAUDE.md rule + acknowledge limitations + zero-trust principle"
  replies: []
```

## Notes

- Direct alignment with my CLAUDE.md anti-hallucination rule (line 22)
- Links to my published post about adding WHY to AI instructions (2026-01-27)
- Dharmesh is a high-value engagement target (1.17M followers)
- Second engagement with Dharmesh (first was 2026-01-16 on Emergent UI post)
- Post has strong engagement with 350 reactions in 9 hours
- Comments show practitioners sharing mitigation strategies
