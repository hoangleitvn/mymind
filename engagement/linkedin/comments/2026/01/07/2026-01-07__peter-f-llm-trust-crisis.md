---
type: linkedin-engagement
created: '2026-01-07T03:37:00Z'
last_updated: '2026-01-07T03:37:00Z'

author:
  name: "Peter F."
  profile: "people/peter-fraedrich.md"
  linkedin_url: "https://www.linkedin.com/in/peterfraedrich"

post:
  source: "external"
  url: "https://www.linkedin.com/posts/peterfraedrich_the-amount-of-trust-that-i-see-people-especially-activity-7414314955119976448-uwaa"
  activity_id: "7414314955119976448"
  date: '2026-01-06'
  reactions: 62
  comments_count: 9
  reposts: 0
  theme: "LLM trust and AI-generated code risks"
  angle: "Blind trust in AI code is tech's 2008 housing crisis"
  key_points:
    - Developers putting too much trust in LLMs
    - AI-generated code without understanding is a massive mistake
    - Companies will hit intrinsic issues when no one understands the codebase
    - Comparing to 2008 housing crisis - unvetted software packaged as quality
  hashtags: []

thread_topic: "AI code generation trust and quality risks"
topic_tags: [ai-coding, llm, code-quality, vibe-coding, technical-debt]

engagement_status: "posted"
response_received: false
follow_up_needed: false
follow_up_date: null
---

## Original Post

The amount of trust that I see people, especially software developers, putting into LLM's is a massive collective FAFO moment for technology. Again, there are situations and use cases where AI can be extremely powerful and useful, but offloading the responsiblity and mental work of writing correct, well-functioning software and having the intimate understanding of its inner workings to AI is a massive mistake. Every company, person, institution, etc. is going to hit a point where they have very large, intrinsic issues with their AI-generated codebase and find out the hard way that no one actually understands how anything works anymore. To me, this is technology's version of the 2008 housing crisis. We're packaging unvetted and low-quality software as if it were the same as pre-AI software, and once the bottom falls out, a lot of regular people are going to be left holding the bag.

## Notable Comments

```yaml
- id: 1736210000000
  author: "Andrew Barkett"
  profile: null
  sentiment: "pushback"
  content: |
I think you're delusional about the past. People have cut and pasted things from Stack Overflow, put massive faith into third party libraries, used open source tools they never even look at, hired random developers in another country they've never seen and accepted their code sight-unseen, etc... for years.
  reactions: 10
  insight: "Valid pushback - blind trust in code isn't new, just amplified"
  replies: []

- id: 1736210100000
  author: "Joseph Muoio"
  profile: null
  sentiment: "balanced"
  content: |
What you're saying is not wrong. It's especially correct for people who just vibe-code more than a prototype. You don't need to treat AI as a blackbox, though. You can use AI and still have an intimate understanding of the code. AI speeds up coding and design but cannot replace human thought or responsibility.
  reactions: 1
  insight: "Balanced take - AI as tool, not replacement for understanding"
  replies: []

- id: 1736210200000
  author: "Katie Hamby"
  profile: null
  sentiment: "agreement"
  content: |
The blind trust of black box AI with limited visibility also into the LLM is astounding to me.
  reactions: 6
  insight: "Agrees with core concern"
  replies: []
```

## Our Engagement

```yaml
- id: 1736232000000
  type: "comment"
  status: "posted"
  sentiment: "balanced"
  content: |
The 2008 analogy is interesting. The risk isn't AI-generated code itself. It's accepting code you don't understand, whether from AI, Stack Overflow, or a contractor.

AI just made it faster to accumulate code nobody owns mentally. The debt compounds quicker now.

The teams I see doing well treat AI output as a first draft, not a finished product. Review it like you would any PR from a junior dev.
  strategy: "Add Angle - reframe the issue as speed of debt accumulation, not AI itself"
  replies:
    - id: 1736300000000
      author: "Andrea Laforgia"
      sentiment: "dismissive"
      content: |
As Hoang Le "It's accepting code you don't understand"

Nonsense. Nobody adopts AI agents that way.
      reactions: 0
      our_response:
        status: "posted"
        content: |
Different experiences then. I've seen teams ship AI-generated code with minimal review - it's common enough that "vibe coding" became a term for it.

If that's not happening in your circles, that's genuinely good to hear.
```

## Notes

- Strong post with good engagement (62 reactions)
- Comments show divided opinions - some pushback that blind trust isn't new
- Aligns with my "vibe coding" concerns and foundation-first thinking
- The 2008 housing crisis analogy is memorable but slightly hyperbolic
- Joseph Muoio's comment about human-in-the-loop is the balanced take
- Andrea Laforgia: dismissive pattern across multiple threads (also on Andrew Ng post), not worth further engagement
  - His core argument: "blind adoption" is a straw man, nobody actually does it, bad engineering existed before AI
  - What he gets right: bad practices did exist pre-AI, "blindly" is rhetorical shorthand, personal responsibility matters
  - Where he's wrong: "nobody adopts blindly" is empirically false (vibe coding exists), scale changes the equation (10x faster accumulation), conflates "nobody should" with "nobody does"
  - Rhetorical pattern: opens with "Nonsense", labels all criticism as straw man, links to his own posts as authority, makes sweeping claims
  - Bottom line: arguing semantics (agency exists) while ignoring empirical reality (poor judgment happens at scale). Not worth engaging.
